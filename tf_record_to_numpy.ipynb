{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converts TFRecord data into numpy. Feel free to modify based on your needs.\n",
    "\n",
    "Data is saved into pickle files. Every file contains a list of samples. # of the samples in a file can be set via config['num_samples_in_numpy_list']. \n",
    "\n",
    "Loading:\n",
    "data = pickle.load(open(<i>path-to-pkl-file</i>, 'rb'))\n",
    "\n",
    "Each sample is a dictionary with the following fields:\n",
    "<ol>\n",
    "  <li>'label': label of the gesture. A unique ID in {0,1,..,19},</li>\n",
    "  <li>'length': length of the gesture sequence, i.e., # of frames,</li>\n",
    "  <li>'depth': tensor of depth images (length, height, width, 1),</li>\n",
    "  <li>'skeleton': tensor of skeleton joints (length, 180),</li>\n",
    "  <li>'rgb': tensor of rgb images (length, height, width, 3),</li>\n",
    "  <li>'segmentation': tensor of segmentation masks (length, height, width, 3).</li>\n",
    "</ol>\n",
    "\n",
    "\n",
    "Note that samples have different number of frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def preprocessing_op(image_op, shape):\n",
    "    \"\"\"\n",
    "    Creates preprocessing operations that are going to be applied on a single frame.\n",
    "    \n",
    "    \"\"\"\n",
    "    with tf.name_scope(\"preprocessing\"):\n",
    "        # Reshape serialized image.\n",
    "        return tf.reshape(image_op, shape)\n",
    "\n",
    "def read_and_decode_sequence(filename_queue, config):\n",
    "    # Create a TFRecordReader.\n",
    "    readerOptions = tf.python_io.TFRecordOptions(compression_type=tf.python_io.TFRecordCompressionType.GZIP)\n",
    "    reader = tf.TFRecordReader(options=readerOptions)\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "    \n",
    "    # Read one sequence sample.\n",
    "    # The training and validation files contains the following fields:\n",
    "    # - label: label of the sequence which take values between 1 and 20.\n",
    "    # - length: length of the sequence, i.e., number of frames.\n",
    "    # - depth: sequence of depth images. [length x height x width x numChannels]\n",
    "    # - rgb: sequence of rgb images. [length x height x width x numChannels]\n",
    "    # - segmentation: sequence of segmentation maskes. [length x height x width x numChannels]\n",
    "    # - skeleton: sequence of flattened skeleton joint positions. [length x numJoints]\n",
    "    #\n",
    "    # The test files doesn't contain \"label\" field.\n",
    "    with tf.name_scope(\"TFRecordDecoding\"):\n",
    "        if train:\n",
    "            context_encoded, sequence_encoded = tf.parse_single_sequence_example(\n",
    "                    serialized_example,\n",
    "                    # \"label\" and \"lenght\" are encoded as context features. \n",
    "                    context_features={\n",
    "                        \"label\": tf.FixedLenFeature([], dtype=tf.int64),\n",
    "                        \"length\": tf.FixedLenFeature([], dtype=tf.int64)\n",
    "                    },\n",
    "                    # \"depth\", \"rgb\", \"segmentation\", \"skeleton\" are encoded as sequence features.\n",
    "                    sequence_features={\n",
    "                        \"depth\": tf.FixedLenSequenceFeature([], dtype=tf.string),\n",
    "                        \"rgb\": tf.FixedLenSequenceFeature([], dtype=tf.string),\n",
    "                        \"segmentation\": tf.FixedLenSequenceFeature([], dtype=tf.string),\n",
    "                        \"skeleton\": tf.FixedLenSequenceFeature([], dtype=tf.string),\n",
    "                    })\n",
    "        else:\n",
    "            context_encoded, sequence_encoded = tf.parse_single_sequence_example(\n",
    "                    serialized_example,\n",
    "                    # \"label\" and \"lenght\" are encoded as context features. \n",
    "                    context_features={\n",
    "                        \"length\": tf.FixedLenFeature([], dtype=tf.int64)\n",
    "                    },\n",
    "                    # \"depth\", \"rgb\", \"segmentation\", \"skeleton\" are encoded as sequence features.\n",
    "                    sequence_features={\n",
    "                        \"depth\": tf.FixedLenSequenceFeature([], dtype=tf.string),\n",
    "                        \"rgb\": tf.FixedLenSequenceFeature([], dtype=tf.string),\n",
    "                        \"segmentation\": tf.FixedLenSequenceFeature([], dtype=tf.string),\n",
    "                        \"skeleton\": tf.FixedLenSequenceFeature([], dtype=tf.string),\n",
    "                    })\n",
    "\n",
    "        # Fetch data fields.\n",
    "        seq_rgb = tf.decode_raw(sequence_encoded['rgb'], tf.uint8)\n",
    "        seq_depth = tf.decode_raw(sequence_encoded['depth'], tf.uint8)\n",
    "        seq_segmentation = tf.decode_raw(sequence_encoded['segmentation'], tf.uint8)\n",
    "        \n",
    "        # Output dimnesionality: [seq_len, height, width, numChannels]\n",
    "        # tf.map_fn applies the preprocessing function on every image in the sequence, i.e., frame.\n",
    "        seq_rgb = tf.map_fn(lambda x: preprocessing_op(x, (config['img_height'], config['img_width'], config['img_num_channels'])),\n",
    "                                elems=seq_rgb,\n",
    "                                dtype=tf.uint8,\n",
    "                                back_prop=False)\n",
    "        seq_depth = tf.map_fn(lambda x: preprocessing_op(x, (config['img_height'], config['img_width'], 1)),\n",
    "                                elems=seq_depth,\n",
    "                                dtype=tf.uint8,\n",
    "                                back_prop=False)\n",
    "        \n",
    "        seq_segmentation = tf.map_fn(lambda x: preprocessing_op(x, (config['img_height'], config['img_width'], config['img_num_channels'])),\n",
    "                                elems=seq_segmentation,\n",
    "                                dtype=tf.uint8,\n",
    "                                back_prop=False)\n",
    "        seq_len = tf.to_int32(context_encoded['length'])\n",
    "        seq_skeleton = tf.decode_raw(sequence_encoded['skeleton'], tf.float32)\n",
    "        if train:\n",
    "            seq_label = context_encoded['label']  \n",
    "        else:\n",
    "            seq_label = 0\n",
    "        \n",
    "        #[batch_size, seq_len, num_skeleton_joints]\n",
    "        \n",
    "        \n",
    "        return [seq_rgb, seq_depth, seq_segmentation, seq_skeleton, seq_label, seq_len]\n",
    "    \n",
    "def input_pipeline(filenames, config):\n",
    "    with tf.name_scope(\"input_pipeline\"):\n",
    "        # Create a queue of TFRecord input files.\n",
    "        filename_queue = tf.train.string_input_producer(filenames, num_epochs=config['num_epochs'], shuffle=False)\n",
    "        # Read the data from TFRecord files, decode and create a list of data samples by using threads.\n",
    "        sample_list = [read_and_decode_sequence(filename_queue, config) for _ in range(config['ip_num_read_threads'])]\n",
    "        # Create batches.\n",
    "\n",
    "        batch_rgb, batch_depth, batch_segmentation,batch_skeleton, \\\n",
    "                    batch_labels, batch_lens = tf.train.batch_join(sample_list,\n",
    "                                                                    batch_size=config['batch_size'],\n",
    "                                                                    capacity=config['ip_queue_capacity'],\n",
    "                                                                    enqueue_many=False,\n",
    "                                                                    dynamic_pad=True,\n",
    "                                                                    allow_smaller_final_batch=True,\n",
    "                                                                    name=\"batch_join_and_pad\")\n",
    "        return batch_rgb, batch_depth, batch_segmentation, batch_skeleton, batch_labels, batch_lens\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = {}\n",
    "# TODO: You can change these fields.\n",
    "train = True #wheter it is train or test. for test, writes a 0 in label\n",
    "#make sure to restart the kernel after switching because it otherwise still uses the old settings\n",
    "config['input_dir'] = \"./train/\" # Directory of the tfrecords.\n",
    "#config['input_dir'] = \"./test/\" # Directory of the tfrecords.\n",
    "config['input_file_format'] = \"dataTrain_%d.tfrecords\" # File naming\n",
    "#config['input_file_format'] = \"dataTest_%d.tfrecords\" # File naming\n",
    "config['input_file_ids'] = list(range(1,41)) # File IDs to be used for training.\n",
    "#config['input_file_ids'] = list(range(1,16)) # File IDs to be used for training.\n",
    "#andres tip: use 16 if validation\n",
    "\n",
    "config['num_samples_in_numpy_list'] = 100 # Put 100 samples in a pickle data file. You can put everything in a single file as well.\n",
    "config['output_dir'] = config['input_dir']\n",
    "config['output_file_format'] = config['input_file_format'].split(\".\")[0]+\".pkl\"\n",
    "config['output_file_start_id'] = 1\n",
    "\n",
    "# Keep these fields fixed.\n",
    "config['img_height'] = 80\n",
    "config['img_width'] = 80\n",
    "config['img_num_channels'] = 3\n",
    "config['num_epochs'] = 1\n",
    "config['batch_size'] = 1\n",
    "# Capacity of the queue which contains the samples read by data readers.\n",
    "# Make sure that it has enough capacity.\n",
    "config['ip_queue_capacity'] = config['batch_size']*10  \n",
    "config['ip_num_read_threads'] = 1\n",
    "# Create a list of TFRecord input files.\n",
    "filenames = [os.path.join(config['input_dir'], config['input_file_format'] % i) for i in config['input_file_ids']]\n",
    "\n",
    "# Create data loading operators. This will be represented as a node in the computational graph.\n",
    "\n",
    "rgb_op, depth_op, segmentation_op, skeleton_op, label_op, seq_len_op = input_pipeline(filenames, config)\n",
    "# Create tensorflow session and initialize the variables (if any).\n",
    "sess = tf.Session()\n",
    "init_op = tf.group(tf.global_variables_initializer(),tf.local_variables_initializer())\n",
    "sess.run(init_op)\n",
    "# Create threads to prefetch the data.\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-c2e336084f2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcoord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mrgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegmentation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskeleton\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrgb_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegmentation_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskeleton_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mnum_samples_read\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mdata_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Andres/miniconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Andres/miniconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Andres/miniconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/Andres/miniconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Andres/miniconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "np_file_id = config['output_file_start_id']\n",
    "output_list = []\n",
    "num_samples_read = 0\n",
    "try:\n",
    "    while not coord.should_stop():\n",
    "        rgb, depth, segmentation, skeleton, label, seq_len = sess.run([rgb_op, depth_op, segmentation_op, skeleton_op, label_op, seq_len_op])\n",
    "        num_samples_read += 1\n",
    "        data_sample = {}\n",
    "        data_sample['rgb'] = rgb[0] # Data is in batch format. Get rid of the first dimension.\n",
    "        data_sample['depth'] = depth[0]\n",
    "        data_sample['segmentation'] = segmentation[0]\n",
    "        data_sample['skeleton'] = skeleton[0]\n",
    "        data_sample['label'] = label[0]\n",
    "        data_sample['length'] = seq_len[0]\n",
    "        output_list.append(data_sample)\n",
    "        \n",
    "        if num_samples_read%config['num_samples_in_numpy_list'] == 0:\n",
    "            pickle.dump(output_list, open(os.path.join(config['output_dir'], config['output_file_format'] % np_file_id), 'wb'))\n",
    "            np_file_id += 1\n",
    "            output_list = []\n",
    "        \n",
    "except tf.errors.OutOfRangeError:\n",
    "    # Save last run.\n",
    "    if len(output_list) > 0:\n",
    "        print(len(output_list))\n",
    "        pickle.dump(output_list, open(os.path.join(config['output_dir'], config['output_file_format'] % np_file_id), 'wb'))\n",
    "        output_list = []\n",
    "    print('Done.')\n",
    "finally:\n",
    "    # When done, ask the threads to stop.\n",
    "    coord.request_stop()\n",
    "\n",
    "# Wait for threads to finish.\n",
    "coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
